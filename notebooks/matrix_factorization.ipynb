{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import IPython\n",
    "from pathlib import Path\n",
    "import os\n",
    "locals = IPython.extract_module_locals() # type: ignore\n",
    "notebook_name = \"/\".join(locals[1][\"__vsc_ipynb_file__\"].split(\"/\"))\n",
    "os.chdir(Path(notebook_name).parent.parent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x1736f2a50>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from typing import Callable, Any\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import polars as pl\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "import random\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "from retail_recommender_system.utils import load_model\n",
    "\n",
    "seed = 0\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "relations = pl.read_parquet(\".data/intermediate/relations.parquet\")\n",
    "users = pl.read_parquet(\".data/intermediate/users.parquet\")\n",
    "items = pl.read_parquet(\".data/intermediate/articles.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "relations = relations.sample(fraction=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_valid, _, _ = train_test_split(relations, np.ones(len(relations)), test_size=0.2, random_state=seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MFDataset(Dataset):\n",
    "    def __init__(self, df: pl.DataFrame, n_items: int, neg_sampl: int = 5):\n",
    "        self._df = df\n",
    "        self._n_items = n_items\n",
    "        self._neg_sampl = neg_sampl\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self._df)\n",
    "    \n",
    "    def __getitem__(self, idx: int) -> dict[str, Any]:\n",
    "        row = self._df[idx]\n",
    "        user = torch.tensor(row.get_column(\"customer_id_map\").to_numpy(), dtype=torch.int32)\n",
    "        items = torch.tensor(row.get_column(\"article_id_map\").to_numpy(), dtype=torch.int32)\n",
    "\n",
    "        u_id = user.repeat(self._neg_sampl + 1)\n",
    "        i_id = torch.cat([items, self._approx_neg_sampl()])\n",
    "        target = torch.tensor([1.0] + [0.0] * self._neg_sampl, dtype=torch.float)\n",
    "\n",
    "        return {\n",
    "            \"u_id\": u_id,\n",
    "            \"i_id\": i_id,\n",
    "            \"target\": target\n",
    "        }\n",
    "    \n",
    "    def _approx_neg_sampl(self):\n",
    "        neg_i_id = torch.randint(low=0, high=self._n_items, size=(self._neg_sampl,), dtype=torch.int32)\n",
    "        return neg_i_id\n",
    "    \n",
    "def _collate_fn(batch):\n",
    "    u_id = torch.cat([x[\"u_id\"] for x in batch])\n",
    "    i_id = torch.cat([x[\"i_id\"] for x in batch])\n",
    "    target = torch.cat([x[\"target\"] for x in batch])\n",
    "    return {\n",
    "        \"u_id\": u_id,\n",
    "        \"i_id\": i_id,\n",
    "        \"target\": target\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MF(nn.Module):\n",
    "    def __init__(self, n_users, n_items, emb_size):\n",
    "        super().__init__()\n",
    "        self.user_factors = nn.Embedding(n_users, emb_size)\n",
    "        self.item_factors = nn.Embedding(n_items, emb_size)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        user_factors = self.user_factors(x[\"u_id\"])\n",
    "        item_factors = self.item_factors(x[\"i_id\"])\n",
    "        return (user_factors * item_factors).sum(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 4096\n",
    "neg_sampl = 3\n",
    "n_users = users.get_column(\"customer_id_map\").max()\n",
    "n_items = items.get_column(\"article_id_map\").max()\n",
    "\n",
    "train_dataset = MFDataset(X_train, n_items=n_items, neg_sampl=neg_sampl)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=_collate_fn)\n",
    "\n",
    "val_dataset = MFDataset(X_valid, n_items=n_items, neg_sampl=neg_sampl)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, collate_fn=_collate_fn)\n",
    "\n",
    "embedding_size = 16\n",
    "dropout_rate = 0.3\n",
    "lr = 1e-4\n",
    "device = torch.device(\"cpu\")\n",
    "\n",
    "model = MF(n_users, n_items, emb_size=embedding_size).to(device)\n",
    "\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _batch_dict_to_device(batch: dict[str, Any], device: torch.device) -> dict[str, Any]:\n",
    "    return {k: v.to(device) for k, v in batch.items()}\n",
    "\n",
    "def train(\n",
    "    model: nn.Module, \n",
    "    loss_fn: Callable,\n",
    "    optimizer: torch.optim.Optimizer,\n",
    "    train_loader: DataLoader,\n",
    "    device: torch.device, \n",
    "    epoch: int, \n",
    "    print_every: None | int = None\n",
    ") -> float:\n",
    "    model.train()\n",
    "    train_loss = 0.\n",
    "\n",
    "    for batch_idx, batch in enumerate(train_loader):\n",
    "        data = _batch_dict_to_device(batch, device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = loss_fn(output, data[\"target\"])\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        loss_item = loss.detach().cpu().item()\n",
    "\n",
    "        if print_every is not None and batch_idx % print_every == 0:\n",
    "            print(\n",
    "                \"Train (Batch): [{}/{} ({:.0f}%)]\\tTrain Loss: {:.4f}\".format(\n",
    "                    batch_idx, len(train_loader), 100.0 * batch_idx / len(train_loader), loss_item\n",
    "                ) # type: ignore\n",
    "            )\n",
    "        train_loss += loss_item\n",
    "\n",
    "    train_loss /= len(train_loader)\n",
    "\n",
    "    return train_loss\n",
    "\n",
    "\n",
    "def test(\n",
    "    model: nn.Module, \n",
    "    loss_fn: Callable,\n",
    "    device: torch.device,\n",
    "    test_loader: DataLoader, \n",
    "    print_every: None | int = None\n",
    ") -> float:\n",
    "    model.eval()\n",
    "    test_loss = 0.\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, batch in enumerate(test_loader):\n",
    "            data = _batch_dict_to_device(batch, device)\n",
    "\n",
    "            output = model(data)\n",
    "            loss =  loss_fn(output, data[\"target\"])\n",
    "\n",
    "            loss_item = loss.detach().cpu().item()\n",
    "            test_loss += loss_item\n",
    "\n",
    "    test_loss /= len(test_loader)\n",
    "\n",
    "    if print_every is not None:\n",
    "        print(\n",
    "            \"\\nTest: Test loss: {:.4f}\".format(test_loss) # type: ignore\n",
    "        )\n",
    "    \n",
    "    return test_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/15 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train (Batch): [0/1242 (0%)]\tTrain Loss: 1.7594\n",
      "Train (Batch): [100/1242 (8%)]\tTrain Loss: 1.7211\n",
      "Train (Batch): [200/1242 (16%)]\tTrain Loss: 1.7101\n",
      "Train (Batch): [300/1242 (24%)]\tTrain Loss: 1.7815\n",
      "Train (Batch): [400/1242 (32%)]\tTrain Loss: 1.7400\n",
      "Train (Batch): [500/1242 (40%)]\tTrain Loss: 1.7403\n",
      "Train (Batch): [600/1242 (48%)]\tTrain Loss: 1.7333\n",
      "Train (Batch): [700/1242 (56%)]\tTrain Loss: 1.7589\n",
      "Train (Batch): [800/1242 (64%)]\tTrain Loss: 1.7600\n",
      "Train (Batch): [900/1242 (72%)]\tTrain Loss: 1.7329\n",
      "Train (Batch): [1000/1242 (81%)]\tTrain Loss: 1.7039\n",
      "Train (Batch): [1100/1242 (89%)]\tTrain Loss: 1.7110\n",
      "Train (Batch): [1200/1242 (97%)]\tTrain Loss: 1.7182\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|â–‹         | 1/15 [02:37<36:42, 157.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test: Test loss: 1.7155\n",
      "Train (Batch): [0/1242 (0%)]\tTrain Loss: 1.7214\n",
      "Train (Batch): [100/1242 (8%)]\tTrain Loss: 1.7141\n",
      "Train (Batch): [200/1242 (16%)]\tTrain Loss: 1.6930\n",
      "Train (Batch): [300/1242 (24%)]\tTrain Loss: 1.6821\n",
      "Train (Batch): [400/1242 (32%)]\tTrain Loss: 1.7004\n",
      "Train (Batch): [500/1242 (40%)]\tTrain Loss: 1.6873\n",
      "Train (Batch): [600/1242 (48%)]\tTrain Loss: 1.7477\n",
      "Train (Batch): [700/1242 (56%)]\tTrain Loss: 1.6902\n",
      "Train (Batch): [800/1242 (64%)]\tTrain Loss: 1.6981\n",
      "Train (Batch): [900/1242 (72%)]\tTrain Loss: 1.6784\n",
      "Train (Batch): [1000/1242 (81%)]\tTrain Loss: 1.6840\n",
      "Train (Batch): [1100/1242 (89%)]\tTrain Loss: 1.6576\n",
      "Train (Batch): [1200/1242 (97%)]\tTrain Loss: 1.6740\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 13%|â–ˆâ–Ž        | 2/15 [05:12<33:47, 156.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test: Test loss: 1.7010\n",
      "Train (Batch): [0/1242 (0%)]\tTrain Loss: 1.6987\n",
      "Train (Batch): [100/1242 (8%)]\tTrain Loss: 1.7223\n",
      "Train (Batch): [200/1242 (16%)]\tTrain Loss: 1.7042\n",
      "Train (Batch): [300/1242 (24%)]\tTrain Loss: 1.6880\n",
      "Train (Batch): [400/1242 (32%)]\tTrain Loss: 1.6768\n",
      "Train (Batch): [500/1242 (40%)]\tTrain Loss: 1.6873\n",
      "Train (Batch): [600/1242 (48%)]\tTrain Loss: 1.6728\n",
      "Train (Batch): [700/1242 (56%)]\tTrain Loss: 1.7012\n",
      "Train (Batch): [800/1242 (64%)]\tTrain Loss: 1.6751\n",
      "Train (Batch): [900/1242 (72%)]\tTrain Loss: 1.6723\n",
      "Train (Batch): [1000/1242 (81%)]\tTrain Loss: 1.6670\n",
      "Train (Batch): [1100/1242 (89%)]\tTrain Loss: 1.6841\n",
      "Train (Batch): [1200/1242 (97%)]\tTrain Loss: 1.6882\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|â–ˆâ–ˆ        | 3/15 [07:49<31:19, 156.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test: Test loss: 1.6866\n",
      "Train (Batch): [0/1242 (0%)]\tTrain Loss: 1.6852\n",
      "Train (Batch): [100/1242 (8%)]\tTrain Loss: 1.7139\n",
      "Train (Batch): [200/1242 (16%)]\tTrain Loss: 1.6851\n",
      "Train (Batch): [300/1242 (24%)]\tTrain Loss: 1.6611\n",
      "Train (Batch): [400/1242 (32%)]\tTrain Loss: 1.6878\n",
      "Train (Batch): [500/1242 (40%)]\tTrain Loss: 1.6636\n",
      "Train (Batch): [600/1242 (48%)]\tTrain Loss: 1.6671\n",
      "Train (Batch): [700/1242 (56%)]\tTrain Loss: 1.7024\n",
      "Train (Batch): [800/1242 (64%)]\tTrain Loss: 1.6537\n",
      "Train (Batch): [900/1242 (72%)]\tTrain Loss: 1.6674\n",
      "Train (Batch): [1000/1242 (81%)]\tTrain Loss: 1.6398\n",
      "Train (Batch): [1100/1242 (89%)]\tTrain Loss: 1.6795\n",
      "Train (Batch): [1200/1242 (97%)]\tTrain Loss: 1.6508\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 27%|â–ˆâ–ˆâ–‹       | 4/15 [10:26<28:45, 156.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test: Test loss: 1.6718\n",
      "Train (Batch): [0/1242 (0%)]\tTrain Loss: 1.6668\n",
      "Train (Batch): [100/1242 (8%)]\tTrain Loss: 1.6789\n",
      "Train (Batch): [200/1242 (16%)]\tTrain Loss: 1.6635\n",
      "Train (Batch): [300/1242 (24%)]\tTrain Loss: 1.6627\n",
      "Train (Batch): [400/1242 (32%)]\tTrain Loss: 1.6572\n",
      "Train (Batch): [500/1242 (40%)]\tTrain Loss: 1.6848\n",
      "Train (Batch): [600/1242 (48%)]\tTrain Loss: 1.6692\n",
      "Train (Batch): [700/1242 (56%)]\tTrain Loss: 1.6426\n",
      "Train (Batch): [800/1242 (64%)]\tTrain Loss: 1.6547\n",
      "Train (Batch): [900/1242 (72%)]\tTrain Loss: 1.6126\n",
      "Train (Batch): [1000/1242 (81%)]\tTrain Loss: 1.6482\n",
      "Train (Batch): [1100/1242 (89%)]\tTrain Loss: 1.6256\n",
      "Train (Batch): [1200/1242 (97%)]\tTrain Loss: 1.6630\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 5/15 [13:04<26:12, 157.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test: Test loss: 1.6569\n",
      "Train (Batch): [0/1242 (0%)]\tTrain Loss: 1.6658\n",
      "Train (Batch): [100/1242 (8%)]\tTrain Loss: 1.6540\n",
      "Train (Batch): [200/1242 (16%)]\tTrain Loss: 1.6663\n",
      "Train (Batch): [300/1242 (24%)]\tTrain Loss: 1.6590\n",
      "Train (Batch): [400/1242 (32%)]\tTrain Loss: 1.6638\n",
      "Train (Batch): [500/1242 (40%)]\tTrain Loss: 1.6369\n",
      "Train (Batch): [600/1242 (48%)]\tTrain Loss: 1.6596\n",
      "Train (Batch): [700/1242 (56%)]\tTrain Loss: 1.6472\n",
      "Train (Batch): [800/1242 (64%)]\tTrain Loss: 1.6613\n",
      "Train (Batch): [900/1242 (72%)]\tTrain Loss: 1.6559\n",
      "Train (Batch): [1000/1242 (81%)]\tTrain Loss: 1.6389\n",
      "Train (Batch): [1100/1242 (89%)]\tTrain Loss: 1.6358\n",
      "Train (Batch): [1200/1242 (97%)]\tTrain Loss: 1.6251\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 6/15 [15:46<23:48, 158.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test: Test loss: 1.6454\n",
      "Train (Batch): [0/1242 (0%)]\tTrain Loss: 1.6090\n",
      "Train (Batch): [100/1242 (8%)]\tTrain Loss: 1.5998\n",
      "Train (Batch): [200/1242 (16%)]\tTrain Loss: 1.6192\n",
      "Train (Batch): [300/1242 (24%)]\tTrain Loss: 1.6372\n",
      "Train (Batch): [400/1242 (32%)]\tTrain Loss: 1.6011\n",
      "Train (Batch): [500/1242 (40%)]\tTrain Loss: 1.6247\n",
      "Train (Batch): [600/1242 (48%)]\tTrain Loss: 1.6157\n",
      "Train (Batch): [700/1242 (56%)]\tTrain Loss: 1.6217\n",
      "Train (Batch): [800/1242 (64%)]\tTrain Loss: 1.6001\n",
      "Train (Batch): [900/1242 (72%)]\tTrain Loss: 1.6187\n",
      "Train (Batch): [1000/1242 (81%)]\tTrain Loss: 1.6316\n",
      "Train (Batch): [1100/1242 (89%)]\tTrain Loss: 1.5858\n",
      "Train (Batch): [1200/1242 (97%)]\tTrain Loss: 1.6152\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 7/15 [18:28<21:18, 159.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test: Test loss: 1.6315\n",
      "Train (Batch): [0/1242 (0%)]\tTrain Loss: 1.5914\n",
      "Train (Batch): [100/1242 (8%)]\tTrain Loss: 1.6072\n",
      "Train (Batch): [200/1242 (16%)]\tTrain Loss: 1.5997\n",
      "Train (Batch): [300/1242 (24%)]\tTrain Loss: 1.6208\n",
      "Train (Batch): [400/1242 (32%)]\tTrain Loss: 1.6165\n",
      "Train (Batch): [500/1242 (40%)]\tTrain Loss: 1.6225\n",
      "Train (Batch): [600/1242 (48%)]\tTrain Loss: 1.6117\n",
      "Train (Batch): [700/1242 (56%)]\tTrain Loss: 1.6106\n",
      "Train (Batch): [800/1242 (64%)]\tTrain Loss: 1.6140\n",
      "Train (Batch): [900/1242 (72%)]\tTrain Loss: 1.5921\n",
      "Train (Batch): [1000/1242 (81%)]\tTrain Loss: 1.5854\n",
      "Train (Batch): [1100/1242 (89%)]\tTrain Loss: 1.6138\n",
      "Train (Batch): [1200/1242 (97%)]\tTrain Loss: 1.6081\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 8/15 [21:07<18:36, 159.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test: Test loss: 1.6162\n",
      "Train (Batch): [0/1242 (0%)]\tTrain Loss: 1.5904\n",
      "Train (Batch): [100/1242 (8%)]\tTrain Loss: 1.6175\n",
      "Train (Batch): [200/1242 (16%)]\tTrain Loss: 1.6210\n",
      "Train (Batch): [300/1242 (24%)]\tTrain Loss: 1.5603\n",
      "Train (Batch): [400/1242 (32%)]\tTrain Loss: 1.5954\n",
      "Train (Batch): [500/1242 (40%)]\tTrain Loss: 1.6053\n",
      "Train (Batch): [600/1242 (48%)]\tTrain Loss: 1.5810\n",
      "Train (Batch): [700/1242 (56%)]\tTrain Loss: 1.6035\n",
      "Train (Batch): [800/1242 (64%)]\tTrain Loss: 1.6180\n",
      "Train (Batch): [900/1242 (72%)]\tTrain Loss: 1.5881\n",
      "Train (Batch): [1000/1242 (81%)]\tTrain Loss: 1.5971\n",
      "Train (Batch): [1100/1242 (89%)]\tTrain Loss: 1.6153\n",
      "Train (Batch): [1200/1242 (97%)]\tTrain Loss: 1.5720\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 9/15 [23:44<15:52, 158.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test: Test loss: 1.6018\n",
      "Train (Batch): [0/1242 (0%)]\tTrain Loss: 1.5884\n",
      "Train (Batch): [100/1242 (8%)]\tTrain Loss: 1.6183\n",
      "Train (Batch): [200/1242 (16%)]\tTrain Loss: 1.6089\n",
      "Train (Batch): [300/1242 (24%)]\tTrain Loss: 1.6242\n",
      "Train (Batch): [400/1242 (32%)]\tTrain Loss: 1.5730\n",
      "Train (Batch): [500/1242 (40%)]\tTrain Loss: 1.5919\n",
      "Train (Batch): [600/1242 (48%)]\tTrain Loss: 1.5865\n",
      "Train (Batch): [700/1242 (56%)]\tTrain Loss: 1.5462\n",
      "Train (Batch): [800/1242 (64%)]\tTrain Loss: 1.5482\n",
      "Train (Batch): [900/1242 (72%)]\tTrain Loss: 1.5676\n",
      "Train (Batch): [1000/1242 (81%)]\tTrain Loss: 1.5660\n",
      "Train (Batch): [1100/1242 (89%)]\tTrain Loss: 1.6169\n",
      "Train (Batch): [1200/1242 (97%)]\tTrain Loss: 1.5834\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 10/15 [26:21<13:10, 158.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test: Test loss: 1.5914\n",
      "Train (Batch): [0/1242 (0%)]\tTrain Loss: 1.5970\n",
      "Train (Batch): [100/1242 (8%)]\tTrain Loss: 1.5704\n",
      "Train (Batch): [200/1242 (16%)]\tTrain Loss: 1.5663\n",
      "Train (Batch): [300/1242 (24%)]\tTrain Loss: 1.5867\n",
      "Train (Batch): [400/1242 (32%)]\tTrain Loss: 1.5651\n",
      "Train (Batch): [500/1242 (40%)]\tTrain Loss: 1.5802\n",
      "Train (Batch): [600/1242 (48%)]\tTrain Loss: 1.5865\n",
      "Train (Batch): [700/1242 (56%)]\tTrain Loss: 1.5638\n",
      "Train (Batch): [800/1242 (64%)]\tTrain Loss: 1.5989\n",
      "Train (Batch): [900/1242 (72%)]\tTrain Loss: 1.5833\n",
      "Train (Batch): [1000/1242 (81%)]\tTrain Loss: 1.5537\n",
      "Train (Batch): [1100/1242 (89%)]\tTrain Loss: 1.5697\n",
      "Train (Batch): [1200/1242 (97%)]\tTrain Loss: 1.5706\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 11/15 [29:00<10:34, 158.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test: Test loss: 1.5755\n",
      "Train (Batch): [0/1242 (0%)]\tTrain Loss: 1.5462\n",
      "Train (Batch): [100/1242 (8%)]\tTrain Loss: 1.5468\n",
      "Train (Batch): [200/1242 (16%)]\tTrain Loss: 1.5387\n",
      "Train (Batch): [300/1242 (24%)]\tTrain Loss: 1.5315\n",
      "Train (Batch): [400/1242 (32%)]\tTrain Loss: 1.5664\n",
      "Train (Batch): [500/1242 (40%)]\tTrain Loss: 1.5454\n",
      "Train (Batch): [600/1242 (48%)]\tTrain Loss: 1.5556\n",
      "Train (Batch): [700/1242 (56%)]\tTrain Loss: 1.5528\n",
      "Train (Batch): [800/1242 (64%)]\tTrain Loss: 1.5625\n",
      "Train (Batch): [900/1242 (72%)]\tTrain Loss: 1.5231\n",
      "Train (Batch): [1000/1242 (81%)]\tTrain Loss: 1.5279\n",
      "Train (Batch): [1100/1242 (89%)]\tTrain Loss: 1.5385\n",
      "Train (Batch): [1200/1242 (97%)]\tTrain Loss: 1.5240\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 12/15 [31:38<07:55, 158.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test: Test loss: 1.5637\n",
      "Train (Batch): [0/1242 (0%)]\tTrain Loss: 1.5032\n",
      "Train (Batch): [100/1242 (8%)]\tTrain Loss: 1.5225\n",
      "Train (Batch): [200/1242 (16%)]\tTrain Loss: 1.5325\n",
      "Train (Batch): [300/1242 (24%)]\tTrain Loss: 1.5167\n",
      "Train (Batch): [400/1242 (32%)]\tTrain Loss: 1.5040\n",
      "Train (Batch): [500/1242 (40%)]\tTrain Loss: 1.5280\n",
      "Train (Batch): [600/1242 (48%)]\tTrain Loss: 1.5486\n",
      "Train (Batch): [700/1242 (56%)]\tTrain Loss: 1.4994\n",
      "Train (Batch): [800/1242 (64%)]\tTrain Loss: 1.5368\n",
      "Train (Batch): [900/1242 (72%)]\tTrain Loss: 1.5377\n",
      "Train (Batch): [1000/1242 (81%)]\tTrain Loss: 1.5105\n",
      "Train (Batch): [1100/1242 (89%)]\tTrain Loss: 1.5364\n",
      "Train (Batch): [1200/1242 (97%)]\tTrain Loss: 1.5137\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 13/15 [34:17<05:17, 158.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test: Test loss: 1.5511\n",
      "Train (Batch): [0/1242 (0%)]\tTrain Loss: 1.5408\n",
      "Train (Batch): [100/1242 (8%)]\tTrain Loss: 1.5249\n",
      "Train (Batch): [200/1242 (16%)]\tTrain Loss: 1.5442\n",
      "Train (Batch): [300/1242 (24%)]\tTrain Loss: 1.5418\n",
      "Train (Batch): [400/1242 (32%)]\tTrain Loss: 1.5637\n",
      "Train (Batch): [500/1242 (40%)]\tTrain Loss: 1.5255\n",
      "Train (Batch): [600/1242 (48%)]\tTrain Loss: 1.5151\n",
      "Train (Batch): [700/1242 (56%)]\tTrain Loss: 1.5523\n",
      "Train (Batch): [800/1242 (64%)]\tTrain Loss: 1.5190\n",
      "Train (Batch): [900/1242 (72%)]\tTrain Loss: 1.5388\n",
      "Train (Batch): [1000/1242 (81%)]\tTrain Loss: 1.5203\n",
      "Train (Batch): [1100/1242 (89%)]\tTrain Loss: 1.4974\n",
      "Train (Batch): [1200/1242 (97%)]\tTrain Loss: 1.5205\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 14/15 [37:00<02:39, 159.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test: Test loss: 1.5370\n",
      "Train (Batch): [0/1242 (0%)]\tTrain Loss: 1.5016\n",
      "Train (Batch): [100/1242 (8%)]\tTrain Loss: 1.5270\n",
      "Train (Batch): [200/1242 (16%)]\tTrain Loss: 1.5124\n",
      "Train (Batch): [300/1242 (24%)]\tTrain Loss: 1.5038\n",
      "Train (Batch): [400/1242 (32%)]\tTrain Loss: 1.5086\n",
      "Train (Batch): [500/1242 (40%)]\tTrain Loss: 1.4946\n",
      "Train (Batch): [600/1242 (48%)]\tTrain Loss: 1.5026\n",
      "Train (Batch): [700/1242 (56%)]\tTrain Loss: 1.5122\n",
      "Train (Batch): [800/1242 (64%)]\tTrain Loss: 1.4979\n",
      "Train (Batch): [900/1242 (72%)]\tTrain Loss: 1.5018\n",
      "Train (Batch): [1000/1242 (81%)]\tTrain Loss: 1.4962\n",
      "Train (Batch): [1100/1242 (89%)]\tTrain Loss: 1.5099\n",
      "Train (Batch): [1200/1242 (97%)]\tTrain Loss: 1.5190\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 15/15 [39:41<00:00, 158.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test: Test loss: 1.5247\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "history = {\n",
    "    'train_loss': [],\n",
    "    'val_loss': []\n",
    "}\n",
    "\n",
    "EPOCHS = 15\n",
    "for epoch in tqdm(range(1, EPOCHS + 1)):\n",
    "    train(model, criterion, optimizer, train_loader, device, epoch, print_every=100)\n",
    "    test(model, criterion, device, val_loader, print_every=1)\n",
    "    #torch.save(model.state_dict(), f\".models/mf_{epoch}.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 2, figsize=(12, 5))\n",
    "ax[0].plot(history['train_loss'], label='Train Loss')\n",
    "ax[0].plot(history['val_loss'], label='Validation Loss')\n",
    "ax[0].set_xlabel('Epochs')\n",
    "ax[0].set_ylabel('Loss')\n",
    "ax[0].set_title('Training and Validation Loss Over Time')\n",
    "\n",
    "ax[1].plot(history['train_rmse'], label='Train RMSE')\n",
    "ax[1].plot(history['val_rmse'], label='Validation RMSE')\n",
    "ax[1].set_xlabel('Epochs')\n",
    "ax[1].set_ylabel('RMSE')\n",
    "ax[1].set_title('Training and Validation RMSE Over Time')\n",
    "\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from retail_recommender_system.evaluation.prediction import recommend_k\n",
    "from retail_recommender_system.evaluation.metrics import precision_k, recall_k\n",
    "from retail_recommender_system.evaluation.evaluation import EvalDataset\n",
    "\n",
    "def collate_fn(batch):\n",
    "    batch = torch.cat(batch)\n",
    "    return {\n",
    "        \"u_id\": batch[:, 0],\n",
    "        \"i_id\": batch[:, 1]\n",
    "    }\n",
    "\n",
    "@torch.no_grad\n",
    "def recommend_udf(batch: dict[str, torch.Tensor], model: nn.Module, K: int = 5) -> torch.Tensor:\n",
    "    model.eval()\n",
    "    return model(batch).view(-1, n_items)\n",
    "\n",
    "K = 5\n",
    "n_users_eval = 1024*20\n",
    "loader = DataLoader(\n",
    "    EvalDataset(n_users_eval, n_items, user_batch_size=1024), \n",
    "    batch_size=1,\n",
    "    collate_fn=collate_fn,\n",
    "    shuffle=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "ground_truth = torch.from_numpy(\n",
    "    (\n",
    "        X_valid\n",
    "        .filter(pl.col(\"customer_id_map\") < n_users_eval)\n",
    "        .select(\"customer_id_map\", \"article_id_map\").to_numpy()\n",
    "    )\n",
    ").to(torch.int32).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_config = {\n",
    "    \"n_users\": n_users,\n",
    "    \"n_items\": n_items,\n",
    "    \"emb_size\": embedding_size   \n",
    "}\n",
    "models = [load_model(MF, Path(f\".models/mf_{i}.pth\"), model_config) for i in range(1, 4)]\n",
    "models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_base = MF(**model_config)\n",
    "models.insert(0, model_base)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, m in enumerate(models):\n",
    "    recommendations = recommend_k(partial(recommend_udf, model=m, K=K), loader, K, past_interactions=None)\n",
    "    precision = precision_k(recommendations, ground_truth, k=K, n_items=n_items)\n",
    "    recall = recall_k(recommendations, ground_truth, k=K, n_items=n_items)\n",
    "    print(f\"Model epoch: {i} - Precision@{K}: {precision:.7f} - Recall@{K}: {recall:.7f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
