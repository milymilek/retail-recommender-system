{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import IPython\n",
    "from pathlib import Path\n",
    "import os\n",
    "locals = IPython.extract_module_locals() # type: ignore\n",
    "notebook_name = \"/\".join(locals[1][\"__vsc_ipynb_file__\"].split(\"/\"))\n",
    "os.chdir(Path(notebook_name).parent.parent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x1327dea90>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from typing import Callable, Any\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import polars as pl\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "import random\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "from retail_recommender_system.utils import load_model\n",
    "\n",
    "seed = 0\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "relations = pl.read_parquet(\".data/intermediate/relations.parquet\")\n",
    "users = pl.read_parquet(\".data/intermediate/users.parquet\")\n",
    "items = pl.read_parquet(\".data/intermediate/articles.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_valid, _, _ = train_test_split(relations, np.ones(len(relations)), test_size=0.2, random_state=seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MFDataset(Dataset):\n",
    "    def __init__(self, df: pl.DataFrame, n_items: int, neg_sampl: int = 5):\n",
    "        self._df = df\n",
    "        self._n_items = n_items\n",
    "        self._neg_sampl = neg_sampl\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self._df)\n",
    "    \n",
    "    def __getitem__(self, idx: int) -> dict[str, Any]:\n",
    "        row = self._df[idx]\n",
    "        user = torch.tensor(row.get_column(\"customer_id_map\").to_numpy(), dtype=torch.int32)\n",
    "        items = torch.tensor(row.get_column(\"article_id_map\").to_numpy(), dtype=torch.int32)\n",
    "\n",
    "        u_id = user.repeat(self._neg_sampl + 1)\n",
    "        i_id = torch.cat([items, self._approx_neg_sampl()])\n",
    "        target = torch.tensor([1.0] + [0.0] * self._neg_sampl, dtype=torch.float)\n",
    "\n",
    "        return {\n",
    "            \"u_id\": u_id,\n",
    "            \"i_id\": i_id,\n",
    "            \"target\": target\n",
    "        }\n",
    "    \n",
    "    def _approx_neg_sampl(self):\n",
    "        neg_i_id = torch.randint(low=0, high=self._n_items, size=(self._neg_sampl,), dtype=torch.int32)\n",
    "        return neg_i_id\n",
    "    \n",
    "def _collate_fn(batch):\n",
    "    u_id = torch.cat([x[\"u_id\"] for x in batch])\n",
    "    i_id = torch.cat([x[\"i_id\"] for x in batch])\n",
    "    target = torch.cat([x[\"target\"] for x in batch])\n",
    "    return {\n",
    "        \"u_id\": u_id,\n",
    "        \"i_id\": i_id,\n",
    "        \"target\": target\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MF(nn.Module):\n",
    "    def __init__(self, n_users, n_items, emb_size):\n",
    "        super().__init__()\n",
    "        self.user_factors = nn.Embedding(n_users, emb_size)\n",
    "        self.item_factors = nn.Embedding(n_items, emb_size)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        user_factors = self.user_factors(x[\"u_id\"])\n",
    "        item_factors = self.item_factors(x[\"i_id\"])\n",
    "        return (user_factors * item_factors).sum(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 4096\n",
    "neg_sampl = 3\n",
    "n_users = relations.select(\"customer_id_map\").max().get_column(\"customer_id_map\").first() + 1\n",
    "n_items = relations.select(\"article_id_map\").max().get_column(\"article_id_map\").first() + 1\n",
    "\n",
    "train_dataset = MFDataset(X_train, n_items=n_items, neg_sampl=neg_sampl)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=_collate_fn)\n",
    "\n",
    "val_dataset = MFDataset(X_valid, n_items=n_items, neg_sampl=neg_sampl)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, collate_fn=_collate_fn)\n",
    "\n",
    "embedding_size = 16\n",
    "dropout_rate = 0.3\n",
    "lr = 1e-4\n",
    "device = torch.device(\"cpu\")\n",
    "\n",
    "model = MF(n_users, n_items, emb_size=embedding_size).to(device)\n",
    "\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _batch_dict_to_device(batch: dict[str, Any], device: torch.device) -> dict[str, Any]:\n",
    "    return {k: v.to(device) for k, v in batch.items()}\n",
    "\n",
    "def train(\n",
    "    model: nn.Module, \n",
    "    loss_fn: Callable,\n",
    "    optimizer: torch.optim.Optimizer,\n",
    "    train_loader: DataLoader,\n",
    "    device: torch.device, \n",
    "    epoch: int, \n",
    "    print_every: None | int = None\n",
    ") -> float:\n",
    "    model.train()\n",
    "    train_loss = 0.\n",
    "\n",
    "    for batch_idx, batch in enumerate(train_loader):\n",
    "        data = _batch_dict_to_device(batch, device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = loss_fn(output, data[\"target\"])\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        loss_item = loss.detach().cpu().item()\n",
    "\n",
    "        if print_every is not None and batch_idx % print_every == 0:\n",
    "            print(\n",
    "                \"Train (Batch): [{}/{} ({:.0f}%)]\\tTrain Loss: {:.4f}\".format(\n",
    "                    batch_idx, len(train_loader), 100.0 * batch_idx / len(train_loader), loss_item\n",
    "                ) # type: ignore\n",
    "            )\n",
    "        train_loss += loss_item\n",
    "\n",
    "    train_loss /= len(train_loader)\n",
    "\n",
    "    return train_loss\n",
    "\n",
    "\n",
    "def test(\n",
    "    model: nn.Module, \n",
    "    loss_fn: Callable,\n",
    "    device: torch.device,\n",
    "    test_loader: DataLoader, \n",
    "    print_every: None | int = None\n",
    ") -> float:\n",
    "    model.eval()\n",
    "    test_loss = 0.\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, batch in enumerate(test_loader):\n",
    "            data = _batch_dict_to_device(batch, device)\n",
    "\n",
    "            output = model(data)\n",
    "            loss =  loss_fn(output, data[\"target\"])\n",
    "\n",
    "            loss_item = loss.detach().cpu().item()\n",
    "            test_loss += loss_item\n",
    "\n",
    "    test_loss /= len(test_loader)\n",
    "\n",
    "    if print_every is not None:\n",
    "        print(\n",
    "            \"\\nTest: Test loss: {:.4f}\".format(test_loss) # type: ignore\n",
    "        )\n",
    "    \n",
    "    return test_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/15 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train (Batch): [0/6209 (0%)]\tTrain Loss: 1.7311\n",
      "Train (Batch): [100/6209 (2%)]\tTrain Loss: 1.7204\n",
      "Train (Batch): [200/6209 (3%)]\tTrain Loss: 1.7287\n",
      "Train (Batch): [300/6209 (5%)]\tTrain Loss: 1.7518\n",
      "Train (Batch): [400/6209 (6%)]\tTrain Loss: 1.7255\n",
      "Train (Batch): [500/6209 (8%)]\tTrain Loss: 1.7460\n",
      "Train (Batch): [600/6209 (10%)]\tTrain Loss: 1.7171\n",
      "Train (Batch): [700/6209 (11%)]\tTrain Loss: 1.7099\n",
      "Train (Batch): [800/6209 (13%)]\tTrain Loss: 1.7268\n",
      "Train (Batch): [900/6209 (14%)]\tTrain Loss: 1.7281\n",
      "Train (Batch): [1000/6209 (16%)]\tTrain Loss: 1.7291\n",
      "Train (Batch): [1100/6209 (18%)]\tTrain Loss: 1.7135\n",
      "Train (Batch): [1200/6209 (19%)]\tTrain Loss: 1.7163\n",
      "Train (Batch): [1300/6209 (21%)]\tTrain Loss: 1.7026\n",
      "Train (Batch): [1400/6209 (23%)]\tTrain Loss: 1.7209\n",
      "Train (Batch): [1500/6209 (24%)]\tTrain Loss: 1.7128\n",
      "Train (Batch): [1600/6209 (26%)]\tTrain Loss: 1.7450\n",
      "Train (Batch): [1700/6209 (27%)]\tTrain Loss: 1.7008\n",
      "Train (Batch): [1800/6209 (29%)]\tTrain Loss: 1.7149\n",
      "Train (Batch): [1900/6209 (31%)]\tTrain Loss: 1.6814\n",
      "Train (Batch): [2000/6209 (32%)]\tTrain Loss: 1.6808\n",
      "Train (Batch): [2100/6209 (34%)]\tTrain Loss: 1.7031\n",
      "Train (Batch): [2200/6209 (35%)]\tTrain Loss: 1.7380\n",
      "Train (Batch): [2300/6209 (37%)]\tTrain Loss: 1.6920\n",
      "Train (Batch): [2400/6209 (39%)]\tTrain Loss: 1.6966\n",
      "Train (Batch): [2500/6209 (40%)]\tTrain Loss: 1.6908\n",
      "Train (Batch): [2600/6209 (42%)]\tTrain Loss: 1.7046\n",
      "Train (Batch): [2700/6209 (43%)]\tTrain Loss: 1.6569\n",
      "Train (Batch): [2800/6209 (45%)]\tTrain Loss: 1.6862\n",
      "Train (Batch): [2900/6209 (47%)]\tTrain Loss: 1.7110\n",
      "Train (Batch): [3000/6209 (48%)]\tTrain Loss: 1.7255\n",
      "Train (Batch): [3100/6209 (50%)]\tTrain Loss: 1.6947\n",
      "Train (Batch): [3200/6209 (52%)]\tTrain Loss: 1.6979\n",
      "Train (Batch): [3300/6209 (53%)]\tTrain Loss: 1.6926\n",
      "Train (Batch): [3400/6209 (55%)]\tTrain Loss: 1.6582\n",
      "Train (Batch): [3500/6209 (56%)]\tTrain Loss: 1.7091\n",
      "Train (Batch): [3600/6209 (58%)]\tTrain Loss: 1.6864\n",
      "Train (Batch): [3700/6209 (60%)]\tTrain Loss: 1.6757\n",
      "Train (Batch): [3800/6209 (61%)]\tTrain Loss: 1.7080\n",
      "Train (Batch): [3900/6209 (63%)]\tTrain Loss: 1.6699\n",
      "Train (Batch): [4000/6209 (64%)]\tTrain Loss: 1.6583\n",
      "Train (Batch): [4100/6209 (66%)]\tTrain Loss: 1.6517\n",
      "Train (Batch): [4200/6209 (68%)]\tTrain Loss: 1.6739\n",
      "Train (Batch): [4300/6209 (69%)]\tTrain Loss: 1.6825\n",
      "Train (Batch): [4400/6209 (71%)]\tTrain Loss: 1.6945\n",
      "Train (Batch): [4500/6209 (72%)]\tTrain Loss: 1.6767\n",
      "Train (Batch): [4600/6209 (74%)]\tTrain Loss: 1.6813\n",
      "Train (Batch): [4700/6209 (76%)]\tTrain Loss: 1.6903\n",
      "Train (Batch): [4800/6209 (77%)]\tTrain Loss: 1.6862\n",
      "Train (Batch): [4900/6209 (79%)]\tTrain Loss: 1.6706\n",
      "Train (Batch): [5000/6209 (81%)]\tTrain Loss: 1.6679\n",
      "Train (Batch): [5100/6209 (82%)]\tTrain Loss: 1.6778\n",
      "Train (Batch): [5200/6209 (84%)]\tTrain Loss: 1.6471\n",
      "Train (Batch): [5300/6209 (85%)]\tTrain Loss: 1.6733\n",
      "Train (Batch): [5400/6209 (87%)]\tTrain Loss: 1.6794\n",
      "Train (Batch): [5500/6209 (89%)]\tTrain Loss: 1.6649\n",
      "Train (Batch): [5600/6209 (90%)]\tTrain Loss: 1.6628\n",
      "Train (Batch): [5700/6209 (92%)]\tTrain Loss: 1.6580\n",
      "Train (Batch): [5800/6209 (93%)]\tTrain Loss: 1.6758\n",
      "Train (Batch): [5900/6209 (95%)]\tTrain Loss: 1.6877\n",
      "Train (Batch): [6000/6209 (97%)]\tTrain Loss: 1.6280\n",
      "Train (Batch): [6100/6209 (98%)]\tTrain Loss: 1.6501\n",
      "Train (Batch): [6200/6209 (100%)]\tTrain Loss: 1.6598\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|▋         | 1/15 [45:43<10:40:11, 2743.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test: Test loss: 1.6580\n",
      "Train (Batch): [0/6209 (0%)]\tTrain Loss: 1.6418\n",
      "Train (Batch): [100/6209 (2%)]\tTrain Loss: 1.6515\n",
      "Train (Batch): [200/6209 (3%)]\tTrain Loss: 1.6763\n",
      "Train (Batch): [300/6209 (5%)]\tTrain Loss: 1.6446\n",
      "Train (Batch): [400/6209 (6%)]\tTrain Loss: 1.6873\n",
      "Train (Batch): [500/6209 (8%)]\tTrain Loss: 1.6756\n",
      "Train (Batch): [600/6209 (10%)]\tTrain Loss: 1.6539\n",
      "Train (Batch): [700/6209 (11%)]\tTrain Loss: 1.6348\n",
      "Train (Batch): [800/6209 (13%)]\tTrain Loss: 1.6542\n",
      "Train (Batch): [900/6209 (14%)]\tTrain Loss: 1.6031\n",
      "Train (Batch): [1000/6209 (16%)]\tTrain Loss: 1.6401\n",
      "Train (Batch): [1100/6209 (18%)]\tTrain Loss: 1.6384\n",
      "Train (Batch): [1200/6209 (19%)]\tTrain Loss: 1.6392\n",
      "Train (Batch): [1300/6209 (21%)]\tTrain Loss: 1.6448\n",
      "Train (Batch): [1400/6209 (23%)]\tTrain Loss: 1.6492\n",
      "Train (Batch): [1500/6209 (24%)]\tTrain Loss: 1.6468\n",
      "Train (Batch): [1600/6209 (26%)]\tTrain Loss: 1.6583\n",
      "Train (Batch): [1700/6209 (27%)]\tTrain Loss: 1.6665\n",
      "Train (Batch): [1800/6209 (29%)]\tTrain Loss: 1.6106\n",
      "Train (Batch): [1900/6209 (31%)]\tTrain Loss: 1.6457\n",
      "Train (Batch): [2000/6209 (32%)]\tTrain Loss: 1.6424\n",
      "Train (Batch): [2100/6209 (34%)]\tTrain Loss: 1.6511\n",
      "Train (Batch): [2200/6209 (35%)]\tTrain Loss: 1.6572\n",
      "Train (Batch): [2300/6209 (37%)]\tTrain Loss: 1.6415\n",
      "Train (Batch): [2400/6209 (39%)]\tTrain Loss: 1.6181\n",
      "Train (Batch): [2500/6209 (40%)]\tTrain Loss: 1.6418\n",
      "Train (Batch): [2600/6209 (42%)]\tTrain Loss: 1.6393\n",
      "Train (Batch): [2700/6209 (43%)]\tTrain Loss: 1.6146\n",
      "Train (Batch): [2800/6209 (45%)]\tTrain Loss: 1.6562\n",
      "Train (Batch): [2900/6209 (47%)]\tTrain Loss: 1.6519\n",
      "Train (Batch): [3000/6209 (48%)]\tTrain Loss: 1.6225\n",
      "Train (Batch): [3100/6209 (50%)]\tTrain Loss: 1.6040\n",
      "Train (Batch): [3200/6209 (52%)]\tTrain Loss: 1.6224\n",
      "Train (Batch): [3300/6209 (53%)]\tTrain Loss: 1.5810\n",
      "Train (Batch): [3400/6209 (55%)]\tTrain Loss: 1.6567\n",
      "Train (Batch): [3500/6209 (56%)]\tTrain Loss: 1.6324\n",
      "Train (Batch): [3600/6209 (58%)]\tTrain Loss: 1.6350\n",
      "Train (Batch): [3700/6209 (60%)]\tTrain Loss: 1.6137\n",
      "Train (Batch): [3800/6209 (61%)]\tTrain Loss: 1.6161\n",
      "Train (Batch): [3900/6209 (63%)]\tTrain Loss: 1.5822\n",
      "Train (Batch): [4000/6209 (64%)]\tTrain Loss: 1.6017\n",
      "Train (Batch): [4100/6209 (66%)]\tTrain Loss: 1.6320\n",
      "Train (Batch): [4200/6209 (68%)]\tTrain Loss: 1.6024\n",
      "Train (Batch): [4300/6209 (69%)]\tTrain Loss: 1.6103\n",
      "Train (Batch): [4400/6209 (71%)]\tTrain Loss: 1.5866\n",
      "Train (Batch): [4500/6209 (72%)]\tTrain Loss: 1.5884\n",
      "Train (Batch): [4600/6209 (74%)]\tTrain Loss: 1.6302\n",
      "Train (Batch): [4700/6209 (76%)]\tTrain Loss: 1.6409\n",
      "Train (Batch): [4800/6209 (77%)]\tTrain Loss: 1.6153\n",
      "Train (Batch): [4900/6209 (79%)]\tTrain Loss: 1.6021\n",
      "Train (Batch): [5000/6209 (81%)]\tTrain Loss: 1.5911\n",
      "Train (Batch): [5100/6209 (82%)]\tTrain Loss: 1.6134\n",
      "Train (Batch): [5200/6209 (84%)]\tTrain Loss: 1.6051\n",
      "Train (Batch): [5300/6209 (85%)]\tTrain Loss: 1.5981\n",
      "Train (Batch): [5400/6209 (87%)]\tTrain Loss: 1.6116\n",
      "Train (Batch): [5500/6209 (89%)]\tTrain Loss: 1.6110\n",
      "Train (Batch): [5600/6209 (90%)]\tTrain Loss: 1.5932\n",
      "Train (Batch): [5700/6209 (92%)]\tTrain Loss: 1.5938\n",
      "Train (Batch): [5800/6209 (93%)]\tTrain Loss: 1.6041\n",
      "Train (Batch): [5900/6209 (95%)]\tTrain Loss: 1.5911\n",
      "Train (Batch): [6000/6209 (97%)]\tTrain Loss: 1.6195\n",
      "Train (Batch): [6100/6209 (98%)]\tTrain Loss: 1.5935\n",
      "Train (Batch): [6200/6209 (100%)]\tTrain Loss: 1.5832\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 13%|█▎        | 2/15 [2:24:52<16:42:59, 4629.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test: Test loss: 1.5888\n",
      "Train (Batch): [0/6209 (0%)]\tTrain Loss: 1.6015\n",
      "Train (Batch): [100/6209 (2%)]\tTrain Loss: 1.6109\n",
      "Train (Batch): [200/6209 (3%)]\tTrain Loss: 1.5330\n",
      "Train (Batch): [300/6209 (5%)]\tTrain Loss: 1.5986\n",
      "Train (Batch): [400/6209 (6%)]\tTrain Loss: 1.5625\n",
      "Train (Batch): [500/6209 (8%)]\tTrain Loss: 1.5627\n",
      "Train (Batch): [600/6209 (10%)]\tTrain Loss: 1.5735\n",
      "Train (Batch): [700/6209 (11%)]\tTrain Loss: 1.5987\n",
      "Train (Batch): [800/6209 (13%)]\tTrain Loss: 1.5558\n",
      "Train (Batch): [900/6209 (14%)]\tTrain Loss: 1.5589\n",
      "Train (Batch): [1000/6209 (16%)]\tTrain Loss: 1.5527\n",
      "Train (Batch): [1100/6209 (18%)]\tTrain Loss: 1.5784\n",
      "Train (Batch): [1200/6209 (19%)]\tTrain Loss: 1.5456\n",
      "Train (Batch): [1300/6209 (21%)]\tTrain Loss: 1.5790\n",
      "Train (Batch): [1400/6209 (23%)]\tTrain Loss: 1.5995\n",
      "Train (Batch): [1500/6209 (24%)]\tTrain Loss: 1.5679\n",
      "Train (Batch): [1600/6209 (26%)]\tTrain Loss: 1.5670\n",
      "Train (Batch): [1700/6209 (27%)]\tTrain Loss: 1.5841\n",
      "Train (Batch): [1800/6209 (29%)]\tTrain Loss: 1.5413\n",
      "Train (Batch): [1900/6209 (31%)]\tTrain Loss: 1.5713\n",
      "Train (Batch): [2000/6209 (32%)]\tTrain Loss: 1.5534\n",
      "Train (Batch): [2100/6209 (34%)]\tTrain Loss: 1.5655\n",
      "Train (Batch): [2200/6209 (35%)]\tTrain Loss: 1.5789\n",
      "Train (Batch): [2300/6209 (37%)]\tTrain Loss: 1.5730\n",
      "Train (Batch): [2400/6209 (39%)]\tTrain Loss: 1.5722\n",
      "Train (Batch): [2500/6209 (40%)]\tTrain Loss: 1.5784\n",
      "Train (Batch): [2600/6209 (42%)]\tTrain Loss: 1.5252\n",
      "Train (Batch): [2700/6209 (43%)]\tTrain Loss: 1.5677\n",
      "Train (Batch): [2800/6209 (45%)]\tTrain Loss: 1.5293\n",
      "Train (Batch): [2900/6209 (47%)]\tTrain Loss: 1.5577\n",
      "Train (Batch): [3000/6209 (48%)]\tTrain Loss: 1.5590\n",
      "Train (Batch): [3100/6209 (50%)]\tTrain Loss: 1.5236\n",
      "Train (Batch): [3200/6209 (52%)]\tTrain Loss: 1.5521\n",
      "Train (Batch): [3300/6209 (53%)]\tTrain Loss: 1.5585\n",
      "Train (Batch): [3400/6209 (55%)]\tTrain Loss: 1.5444\n",
      "Train (Batch): [3500/6209 (56%)]\tTrain Loss: 1.5496\n",
      "Train (Batch): [3600/6209 (58%)]\tTrain Loss: 1.5539\n",
      "Train (Batch): [3700/6209 (60%)]\tTrain Loss: 1.5346\n",
      "Train (Batch): [3800/6209 (61%)]\tTrain Loss: 1.5460\n",
      "Train (Batch): [3900/6209 (63%)]\tTrain Loss: 1.5614\n",
      "Train (Batch): [4000/6209 (64%)]\tTrain Loss: 1.5414\n",
      "Train (Batch): [4100/6209 (66%)]\tTrain Loss: 1.5187\n",
      "Train (Batch): [4200/6209 (68%)]\tTrain Loss: 1.5534\n",
      "Train (Batch): [4300/6209 (69%)]\tTrain Loss: 1.5362\n",
      "Train (Batch): [4400/6209 (71%)]\tTrain Loss: 1.5224\n",
      "Train (Batch): [4500/6209 (72%)]\tTrain Loss: 1.5424\n",
      "Train (Batch): [4600/6209 (74%)]\tTrain Loss: 1.5123\n",
      "Train (Batch): [4700/6209 (76%)]\tTrain Loss: 1.5700\n",
      "Train (Batch): [4800/6209 (77%)]\tTrain Loss: 1.5420\n",
      "Train (Batch): [4900/6209 (79%)]\tTrain Loss: 1.5086\n",
      "Train (Batch): [5000/6209 (81%)]\tTrain Loss: 1.5326\n",
      "Train (Batch): [5100/6209 (82%)]\tTrain Loss: 1.5166\n",
      "Train (Batch): [5200/6209 (84%)]\tTrain Loss: 1.5303\n",
      "Train (Batch): [5300/6209 (85%)]\tTrain Loss: 1.5241\n",
      "Train (Batch): [5400/6209 (87%)]\tTrain Loss: 1.5447\n",
      "Train (Batch): [5500/6209 (89%)]\tTrain Loss: 1.5022\n",
      "Train (Batch): [5600/6209 (90%)]\tTrain Loss: 1.5159\n",
      "Train (Batch): [5700/6209 (92%)]\tTrain Loss: 1.5157\n",
      "Train (Batch): [5800/6209 (93%)]\tTrain Loss: 1.5442\n",
      "Train (Batch): [5900/6209 (95%)]\tTrain Loss: 1.5347\n",
      "Train (Batch): [6000/6209 (97%)]\tTrain Loss: 1.5093\n",
      "Train (Batch): [6100/6209 (98%)]\tTrain Loss: 1.5237\n",
      "Train (Batch): [6200/6209 (100%)]\tTrain Loss: 1.5156\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 3/15 [2:40:58<9:51:14, 2956.18s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test: Test loss: 1.5216\n",
      "Train (Batch): [0/6209 (0%)]\tTrain Loss: 1.5184\n",
      "Train (Batch): [100/6209 (2%)]\tTrain Loss: 1.4999\n",
      "Train (Batch): [200/6209 (3%)]\tTrain Loss: 1.5171\n",
      "Train (Batch): [300/6209 (5%)]\tTrain Loss: 1.5234\n",
      "Train (Batch): [400/6209 (6%)]\tTrain Loss: 1.5165\n",
      "Train (Batch): [500/6209 (8%)]\tTrain Loss: 1.5143\n",
      "Train (Batch): [600/6209 (10%)]\tTrain Loss: 1.5046\n",
      "Train (Batch): [700/6209 (11%)]\tTrain Loss: 1.5166\n",
      "Train (Batch): [800/6209 (13%)]\tTrain Loss: 1.4919\n",
      "Train (Batch): [900/6209 (14%)]\tTrain Loss: 1.5033\n",
      "Train (Batch): [1000/6209 (16%)]\tTrain Loss: 1.4976\n",
      "Train (Batch): [1100/6209 (18%)]\tTrain Loss: 1.4959\n",
      "Train (Batch): [1200/6209 (19%)]\tTrain Loss: 1.4895\n",
      "Train (Batch): [1300/6209 (21%)]\tTrain Loss: 1.4877\n",
      "Train (Batch): [1400/6209 (23%)]\tTrain Loss: 1.5031\n",
      "Train (Batch): [1500/6209 (24%)]\tTrain Loss: 1.4896\n",
      "Train (Batch): [1600/6209 (26%)]\tTrain Loss: 1.5091\n",
      "Train (Batch): [1700/6209 (27%)]\tTrain Loss: 1.5035\n",
      "Train (Batch): [1800/6209 (29%)]\tTrain Loss: 1.5106\n",
      "Train (Batch): [1900/6209 (31%)]\tTrain Loss: 1.4835\n",
      "Train (Batch): [2000/6209 (32%)]\tTrain Loss: 1.4735\n",
      "Train (Batch): [2100/6209 (34%)]\tTrain Loss: 1.4839\n",
      "Train (Batch): [2200/6209 (35%)]\tTrain Loss: 1.4790\n",
      "Train (Batch): [2300/6209 (37%)]\tTrain Loss: 1.4987\n",
      "Train (Batch): [2400/6209 (39%)]\tTrain Loss: 1.5024\n",
      "Train (Batch): [2500/6209 (40%)]\tTrain Loss: 1.5030\n",
      "Train (Batch): [2600/6209 (42%)]\tTrain Loss: 1.4929\n",
      "Train (Batch): [2700/6209 (43%)]\tTrain Loss: 1.4890\n",
      "Train (Batch): [2800/6209 (45%)]\tTrain Loss: 1.4839\n",
      "Train (Batch): [2900/6209 (47%)]\tTrain Loss: 1.4860\n",
      "Train (Batch): [3000/6209 (48%)]\tTrain Loss: 1.4803\n",
      "Train (Batch): [3100/6209 (50%)]\tTrain Loss: 1.4996\n",
      "Train (Batch): [3200/6209 (52%)]\tTrain Loss: 1.4695\n",
      "Train (Batch): [3300/6209 (53%)]\tTrain Loss: 1.4635\n",
      "Train (Batch): [3400/6209 (55%)]\tTrain Loss: 1.4958\n",
      "Train (Batch): [3500/6209 (56%)]\tTrain Loss: 1.4684\n",
      "Train (Batch): [3600/6209 (58%)]\tTrain Loss: 1.5070\n",
      "Train (Batch): [3700/6209 (60%)]\tTrain Loss: 1.4861\n",
      "Train (Batch): [3800/6209 (61%)]\tTrain Loss: 1.5026\n",
      "Train (Batch): [3900/6209 (63%)]\tTrain Loss: 1.4694\n",
      "Train (Batch): [4000/6209 (64%)]\tTrain Loss: 1.4886\n",
      "Train (Batch): [4100/6209 (66%)]\tTrain Loss: 1.4550\n",
      "Train (Batch): [4200/6209 (68%)]\tTrain Loss: 1.4453\n",
      "Train (Batch): [4300/6209 (69%)]\tTrain Loss: 1.4884\n",
      "Train (Batch): [4400/6209 (71%)]\tTrain Loss: 1.4682\n",
      "Train (Batch): [4500/6209 (72%)]\tTrain Loss: 1.4768\n",
      "Train (Batch): [4600/6209 (74%)]\tTrain Loss: 1.4796\n",
      "Train (Batch): [4700/6209 (76%)]\tTrain Loss: 1.4762\n",
      "Train (Batch): [4800/6209 (77%)]\tTrain Loss: 1.4838\n",
      "Train (Batch): [4900/6209 (79%)]\tTrain Loss: 1.4677\n",
      "Train (Batch): [5000/6209 (81%)]\tTrain Loss: 1.4486\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 3/15 [3:21:21<13:25:27, 4027.26s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 8\u001b[0m\n\u001b[1;32m      6\u001b[0m EPOCHS \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m15\u001b[39m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m tqdm(\u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, EPOCHS \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m)):\n\u001b[0;32m----> 8\u001b[0m     \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprint_every\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      9\u001b[0m     test(model, criterion, device, val_loader, print_every\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     10\u001b[0m     torch\u001b[38;5;241m.\u001b[39msave(model\u001b[38;5;241m.\u001b[39mstate_dict(), \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.models/mf_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.pth\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[18], line 16\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, loss_fn, optimizer, train_loader, device, epoch, print_every)\u001b[0m\n\u001b[1;32m     13\u001b[0m model\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[1;32m     14\u001b[0m train_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.\u001b[39m\n\u001b[0;32m---> 16\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch_idx, batch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(train_loader):\n\u001b[1;32m     17\u001b[0m     data \u001b[38;5;241m=\u001b[39m _batch_dict_to_device(batch, device)\n\u001b[1;32m     19\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n",
      "File \u001b[0;32m~/Documents/uv/MOW_2/lab/retail_recommender_system/.venv/lib/python3.10/site-packages/torch/utils/data/dataloader.py:701\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    698\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    699\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    700\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 701\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    702\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    703\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    704\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable\n\u001b[1;32m    705\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    706\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called\n\u001b[1;32m    707\u001b[0m ):\n",
      "File \u001b[0;32m~/Documents/uv/MOW_2/lab/retail_recommender_system/.venv/lib/python3.10/site-packages/torch/utils/data/dataloader.py:757\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    755\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    756\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 757\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    758\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    759\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m~/Documents/uv/MOW_2/lab/retail_recommender_system/.venv/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:55\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n\u001b[0;32m---> 55\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollate_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[15], line 32\u001b[0m, in \u001b[0;36m_collate_fn\u001b[0;34m(batch)\u001b[0m\n\u001b[1;32m     30\u001b[0m u_id \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat([x[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mu_id\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m batch])\n\u001b[1;32m     31\u001b[0m i_id \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat([x[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mi_id\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m batch])\n\u001b[0;32m---> 32\u001b[0m target \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mx\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtarget\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m {\n\u001b[1;32m     34\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mu_id\u001b[39m\u001b[38;5;124m\"\u001b[39m: u_id,\n\u001b[1;32m     35\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mi_id\u001b[39m\u001b[38;5;124m\"\u001b[39m: i_id,\n\u001b[1;32m     36\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtarget\u001b[39m\u001b[38;5;124m\"\u001b[39m: target\n\u001b[1;32m     37\u001b[0m }\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "history = {\n",
    "    'train_loss': [],\n",
    "    'val_loss': []\n",
    "}\n",
    "\n",
    "EPOCHS = 15\n",
    "for epoch in tqdm(range(1, EPOCHS + 1)):\n",
    "    train(model, criterion, optimizer, train_loader, device, epoch, print_every=100)\n",
    "    test(model, criterion, device, val_loader, print_every=1)\n",
    "    torch.save(model.state_dict(), f\".models/mf_{epoch}.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 2, figsize=(12, 5))\n",
    "ax[0].plot(history['train_loss'], label='Train Loss')\n",
    "ax[0].plot(history['val_loss'], label='Validation Loss')\n",
    "ax[0].set_xlabel('Epochs')\n",
    "ax[0].set_ylabel('Loss')\n",
    "ax[0].set_title('Training and Validation Loss Over Time')\n",
    "\n",
    "ax[1].plot(history['train_rmse'], label='Train RMSE')\n",
    "ax[1].plot(history['val_rmse'], label='Validation RMSE')\n",
    "ax[1].set_xlabel('Epochs')\n",
    "ax[1].set_ylabel('RMSE')\n",
    "ax[1].set_title('Training and Validation RMSE Over Time')\n",
    "\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 95%|█████████▌| 20/21 [00:56<00:02,  2.83s/it]\n"
     ]
    }
   ],
   "source": [
    "from functools import partial\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from retail_recommender_system.evaluation.prediction import recommend_k\n",
    "from retail_recommender_system.evaluation.metrics import precision_k, recall_k\n",
    "from retail_recommender_system.evaluation.evaluation import EvalDataset\n",
    "\n",
    "def collate_fn(batch):\n",
    "    batch = torch.cat(batch)\n",
    "    return {\n",
    "        \"u_id\": batch[:, 0],\n",
    "        \"i_id\": batch[:, 1]\n",
    "    }\n",
    "\n",
    "@torch.no_grad\n",
    "def recommend_udf(model: nn.Module, batch: dict[str, torch.Tensor], K=5) -> torch.Tensor:\n",
    "    model.eval()\n",
    "    return model(batch).view(-1, n_items)\n",
    "\n",
    "K = 5\n",
    "n_users_eval = 1024*20\n",
    "loader = DataLoader(\n",
    "    EvalDataset(n_users_eval, n_items, user_batch_size=1024), \n",
    "    batch_size=1,\n",
    "    collate_fn=collate_fn,\n",
    "    shuffle=False\n",
    ")\n",
    "recommendations = recommend_k(partial(recommend_udf, model=model, K=K), loader, K, past_interactions=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "ground_truth = torch.from_numpy(\n",
    "    (\n",
    "        X_valid\n",
    "        .filter(pl.col(\"customer_id_map\") < n_users_eval)\n",
    "        .select(\"customer_id_map\", \"article_id_map\").to_numpy()\n",
    "    )\n",
    ").to(torch.int32).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(9.1271e-05)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "precision_k(recommendations, ground_truth, k=K, n_items=n_items)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(4.2958e-05)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recall_k(recommendations, ground_truth, k=K, n_items=n_items)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
